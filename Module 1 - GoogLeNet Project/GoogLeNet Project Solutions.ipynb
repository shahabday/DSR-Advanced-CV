{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogLeNet for Cats and Dogs predictions\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://www.allaboutpetsprovo.com/wp-content/uploads/2019/09/cat-dog-exchange.jpg)\n",
    "<center>Image taken from <a href=\"https://www.allaboutpetsprovo.com/cats-vs-dogs.html\">here</a></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In this lesson, you will build GoogLeNet neural network from **scratch** using the Keras (TensorFlow 2.+) library and train it to recognize images of cats and dogs. Let's start!\n",
    "\n",
    "### Steps:\n",
    "1. Import libraries and download the dataset\n",
    "2. Create an InceptionBlock \n",
    "3. Build the original GoogLeNet architecture\n",
    "4. Load data using tensorflow ImageDataGenerator\n",
    "5. Train the model\n",
    "\n",
    "### Topics covered and learning objectives\n",
    "- Load image data from folders using *ImageDataGenerators*\n",
    "- GoogLeNet model - Implementation and network architecture\n",
    "- Inception blocks\n",
    "- Build from scratch GoogLeNet model using Keras (TensorFlow) library\n",
    "\n",
    "### Time estimates:\n",
    "- Reading/Watching materials: 1h 45min\n",
    "- Exercises: 1h 10min\n",
    "<br><br>\n",
    "- **Total**: ~3h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PurePath, Path\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tests import *\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, Layer, MaxPool2D, GlobalAvgPool2D, Dense, AveragePooling2D, Flatten, Dropout, Input\n",
    "\n",
    "# For loading YouTube videos\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "Before starting the project, you'll need to download it from Kaggle and place it inside the **data**  folder created for you. [Download here](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "\n",
    "For this exercise, we will use 2,000 images, which is only a subset of the entire dataset of 25,000 images.\n",
    "\n",
    "![](images/download.png)\n",
    "\n",
    "NOTE: Download might take a while! It is about 800 MBs\n",
    "\n",
    "Once downloaded, your zip file will contain **two (2)** zip files.\n",
    "\n",
    "Extract only these two files:\n",
    "- train.zip\n",
    "- test1.zip\n",
    "\n",
    "Extract both of them in the **data/module_1** folder inside the root directory.\n",
    "\n",
    "After extracting everything, this was my folder structure:\n",
    "\n",
    "<pre>\n",
    "<b>module_1</b>\n",
    "|__ <b>train</b>\n",
    "    |______ <b>cats</b>: [cat.0.jpg, cat.1.jpg, cat.2.jpg ...]\n",
    "    |______ <b>dogs</b>: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]\n",
    "|__ <b>test1</b>\n",
    "    |______ <b>cats</b>: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ...]\n",
    "    |______ <b>dogs</b>: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]\n",
    "</pre>\n",
    "\n",
    "If everything is okay with this step, let's go and build the first part of our network, the InceptionBlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths setup - used later in the code. You don't have to change anything here\n",
    "REPO_DIR = Path(os.getcwd()).parent\n",
    "\n",
    "# Note: Please put the data into the data folder in the root of the repo for the following to work!\n",
    "train_dir = REPO_DIR / \"data/module_1/train\"\n",
    "validation_dir = REPO_DIR / \"data/module_1/test1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inception block](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-17-11-14-10.png)\n",
    "<center>Image taken from <a href=\"https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/\">here</a></center>\n",
    "\n",
    "<br><br>\n",
    "As seen on the image above, the Inception Block has 4 different parts which analyze an image (or input to the block) in different ways. \n",
    "\n",
    "Instead of having only one size representation of a layer input, Inception Block allows us to extract features from different image sizes, make our network more robust and ultimately more accurate.\n",
    "\n",
    "Okay, but what is the architecture?\n",
    "It's straightforward to build.\n",
    "\n",
    "#### 1st part\n",
    "- One conv layer with a kernel size of 1, ReLu activation\n",
    "\n",
    "#### 2nd part\n",
    "- First, conv layer with a kernel size of 1, ReLu activation\n",
    "- Second, conv layer with a kernel size of 3, ReLu activation and padding same\n",
    "\n",
    "#### 3rd part\n",
    "- First, conv layer with a kernel size of 1, ReLu activation\n",
    "- Second, conv layer with a kernel size of 5, ReLu activation and padding same\n",
    "\n",
    "#### 4rt part\n",
    "- First, MaxPool layer with a pool size of 3\n",
    "- Second, conv layer with a kernel size of 1, ReLu activation\n",
    "\n",
    "#### 5th part\n",
    "- Return the concatenation of all 4 channels using tf.concat\n",
    "\n",
    "Links to learn more about Inception blocks:\n",
    "\n",
    "Reading:\n",
    "- https://paperswithcode.com/method/inception-module \n",
    "- https://deepai.org/machine-learning-glossary-and-terms/inception-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30738c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/C86ZXvgpejM\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222237d0",
   "metadata": {},
   "source": [
    "**In some cases Ipython widgets do not work!**\n",
    "\n",
    "If this is the case here is the like for YouTube video from cell above: https://www.youtube.com/watch?v=C86ZXvgpejM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/KfV8CJh7hE0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ff3697c4c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/KfV8CJh7hE0\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In some cases Ipython widgets do not work!**\n",
    "\n",
    "If this is the case here is the like for YouTube video from cell above: https://www.youtube.com/embed/KfV8CJh7hE0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/STTrebkhnIk\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ff3d13c908>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/STTrebkhnIk\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In some cases Ipython widgets do not work!**\n",
    "\n",
    "If this is the case here is the like for YouTube video from cell above: https://www.youtube.com/embed/STTrebkhnIk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 1:\n",
    "\n",
    "Using the explanations and resources provided, complete the **InceptionBlock** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionBlock(inputs, filters_1, filters_2, filters_3, filters_4):\n",
    "    \n",
    "    # Path 1 \n",
    "    conv1 = Conv2D(filters_1, kernel_size=1, activation='relu')(inputs)\n",
    "    \n",
    "    # Path 2 \n",
    "    conv2_1 = Conv2D(filters_2[0], kernel_size=1, activation='relu')(inputs)\n",
    "    conv2_2 = Conv2D(filters_2[1], kernel_size=3, padding='same',\n",
    "                          activation='relu')(conv2_1)\n",
    "    \n",
    "    # Path 3 \n",
    "    conv3_1 = Conv2D(filters_3[0], kernel_size=1, activation='relu')(inputs)\n",
    "    conv3_2 = Conv2D(filters_3[1], kernel_size=5, padding='same',\n",
    "                          activation='relu')(conv3_1)\n",
    "    \n",
    "    # Path 4 \n",
    "    maxpool = MaxPool2D(pool_size=3, strides=1, padding='same')(inputs)\n",
    "    conv4_2 = Conv2D(filters_4, kernel_size=1, activation='relu')(maxpool)\n",
    "    \n",
    "    return tf.concat([conv1, conv2_2, conv3_2, conv4_2], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:green>IMPLEMENTATION IS CORRECT! GOOD JOB!</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO CHECK IF YOUR SOLUTION IS CORRECT\n",
    "TEST_INCEPTIONBLOCK(InceptionBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogleLeNet\n",
    "\n",
    "### Implementing GoogLeNet from scratch\n",
    "\n",
    "Like all big and famous architectures, GoogLeNet was created for the ImageNet competition. This architecture was later used to develop SOTA Face recognition applications, Reverse Image search, and many other Google products.\n",
    "\n",
    "What is so special about this model?\n",
    "\n",
    "GoogLeNet was created to solve the overfitting problem of big architectures. This was achieved by using Inception modules (layers) instead of the regular ones. Besides this *trick*, the authors have added two **mini-networks** in the middle of the model. These mini-networks are called Auxiliary classifiers.\n",
    "\n",
    "Read this -> https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5\n",
    "\n",
    "### Auxiliary classifier\n",
    "\n",
    "Auxiliary classifiers are small networks used ONLY in the TRAINING time to prevent vanishing gradient problems for more extensive networks. \n",
    "\n",
    "These small networks have the same output layer as the primary (big) model, with the Softmax/Sigmoid function. Calculating loss from these points helps preserver gradients in lower layers in the model and update the training time better. \n",
    "\n",
    "Note: The number of outputs depends on the number of classes. Our task here is cats vs. dogs. Since this is the binary classification, we will use Sigmoid instead of Softmax with only 1 (one) neuron as the output.\n",
    "\n",
    "While this is awesome for the training process, it is useless for the inference time, so we would only keep the main model in the production.\n",
    "\n",
    "Links to learn more about Auxiliary classifiers:\n",
    "\n",
    "- https://towardsdatascience.com/deep-learning-googlenet-explained-de8861c82765\n",
    "\n",
    "![Auxiliary classifier](https://miro.medium.com/max/550/1*htr2D6tKh3JMS7Acy4BDTw.png)\n",
    "<center>Image taken from <a href=\"https://towardsdatascience.com/deep-learning-googlenet-explained-de8861c82765\">here</a></center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The architecture of the Auxiliary Classifier is pretty simple.\n",
    "- Start with AveragePooling with a pool size of 5x5 and strides of 3\n",
    "- Put that through Conv layer with 128 feature maps, kernel size of 1, padding same, and activation relu\n",
    "- Flatten the output of the Conv layer\n",
    "- Use Dense layer with activation relu and 1024 units\n",
    "- Add dropout layer of 0.7 or 70% drop\n",
    "- Complete it with a Dense (output) layer with 1 unit for binary classification or with the same number as the number of classes for multi-class classification.  (Sigmoid or Softmax)\n",
    "\n",
    "### Exercise 2 Complete the Auxiliary Classifier function\n",
    "\n",
    "Using the explanation and links provided, complete the *AuxiliaryClassifier* function and run tests to check if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AuxiliaryClassifier(X):\n",
    "    X = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n",
    "    X = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1024, activation = 'relu')(X)\n",
    "    X = Dropout(0.7)(X)\n",
    "    X = Dense(1, activation = 'sigmoid')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:green>IMPLEMENTATION IS CORRECT! GOOD JOB!</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO CHECK IF YOUR IMPLEMENTATION IS CORRECT\n",
    "TEST_AUXILARY(AuxilaryClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet architecture\n",
    "\n",
    "![GoogleNet model](https://paperswithcode.com/media/methods/Screen_Shot_2020-06-22_at_3.28.59_PM.png)\n",
    "<center>Image taken from <a href=\"https://paperswithcode.com\">here</a></center>\n",
    "<br><br>\n",
    "Now that we have the most crucial components of the GoogleNet model (**InceptionBlock** and **AuxiliaryClassifier**), let's walk through the whole architecture and start by implementing it inside the **GoogLeNet function**.\n",
    "\n",
    "GoogLeNet implementation guide:\n",
    "\n",
    "1. Start by defining the Input layer. In the original paper, the model accepted (224, 224, 3) size, so let's keep that.\n",
    "2. Define the first part of the model that goes:\n",
    "    - Conv with 64 feature maps, Kernel size of 7 and strides of 2, padding=valid\n",
    "    - Followed by MaxPooling layer with pooling size of 3 and strides 2, padding = same\n",
    "    - Conv with 64 feature maps with a kernel size of 1\n",
    "    - Conv with 192 feature maps, kernel size of 3, and padding is the same\n",
    "    - Finish this part with MaxPooling with a kernel size of 3 and strides of 2\n",
    "    \n",
    "3.  This part is given to you as a reference in the GoogLeNet function\n",
    "4.  Define the first Auxiliary Classifier \n",
    "5.  Followed by 3 Inception Blocks\n",
    "    - 1st block: 160, (112, 224), (24, 64), 64\n",
    "    - 2nd block: 128, (128, 256), (24, 64), 64\n",
    "    - 3rd block: 112, (144, 288), (32, 64), 64\n",
    "6. Define the second Auxiliary Classifier\n",
    "7. Define the last part of the network\n",
    "    - Inception block with config: 256, (160, 320), (32, 128), 128\n",
    "    - MaxPooling layer with pooling size of 3, strides are 2, and padding is same\n",
    "    - Inception block: 256, (160, 320), (32, 128), 128\n",
    "    - Inception block: 384, (192, 384), (48, 128), 128\n",
    "    - Global Average pooling layer\n",
    "    - Complete the network with Dense layer with the number of units 1 (Dogs vs. cats), activation sigmoid, and name=\"output\"\n",
    "\n",
    "8. Define the model using keras Model, where inputs will be inputs defined from the 1st step, and the outputs will be a list of 3 things - Last layer of the model, auxiliary classifier 1 outputs, and auxiliary classifier 2 outputs\n",
    "\n",
    "Learn more about GoogLeNet: \n",
    "- https://towardsdatascience.com/deep-learning-googlenet-explained-de8861c82765\n",
    "- https://www.geeksforgeeks.org/understanding-googlenet-model-cnn-architecture/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoogLeNet():\n",
    "    \n",
    "    inputs = Input(shape = (224, 224, 3))\n",
    "    \n",
    "    X = Conv2D(64, kernel_size=7, strides=2, padding='valid', activation='relu')(inputs)\n",
    "    \n",
    "    X = MaxPool2D(pool_size=3, strides=2, padding='same')(X)\n",
    "    \n",
    "    X = Conv2D(64, kernel_size=1, activation='relu')(X)\n",
    "    \n",
    "    X = Conv2D(192, kernel_size=3, padding='same', activation='relu')(X)\n",
    "    X = MaxPool2D(pool_size=3, strides=2)(X)\n",
    "    \n",
    "    X = InceptionBlock(X, 64, (96, 128), (16, 32), 32)\n",
    "    X = InceptionBlock(X, 128, (128, 192), (32, 96), 64)\n",
    "    X = MaxPool2D(pool_size=3, strides=2)(X)\n",
    "    X = InceptionBlock(X, 192, (96, 208), (16, 48), 64)\n",
    "\n",
    "    aux_output = AuxiliaryClassifier(X)\n",
    "      \n",
    "    X = InceptionBlock(X, 160, (112, 224), (24, 64), 64)\n",
    "    X = InceptionBlock(X, 128, (128, 256), (24, 64), 64)\n",
    "    X = InceptionBlock(X, 112, (144, 288), (32, 64), 64)\n",
    "    \n",
    "    aux_output2 = AuxiliaryClassifier(X)\n",
    "    \n",
    "    \n",
    "    X = InceptionBlock(X, 256, (160, 320), (32, 128), 128)\n",
    "    X = MaxPool2D(pool_size=3, strides=2, padding='same')(X)\n",
    "    X = InceptionBlock(X, 256, (160, 320), (32, 128), 128)\n",
    "    X = InceptionBlock(X, 384, (192, 384), (48, 128), 128)\n",
    "    X = GlobalAvgPool2D()(X)\n",
    "    X = Dense(1, activation='sigmoid', name='output')(X)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[X, aux_output, aux_output2])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use your completed function GoogLeNet() and defined the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GoogLeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:green>IMPLEMENTATION IS CORRECT! GOOD JOB!</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO CHECK IF YOUR IMPLEMENTATION OF GOOGLENET IS CORRECT\n",
    "TEST_GOOGLENET(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['binary_crossentropy', 'binary_crossentropy', 'binary_crossentropy'], \n",
    "              loss_weights=[1, 0.3, 0.3],\n",
    "              optimizer=RMSprop(lr=0.001),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up config (Hyperparams) for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=(224, 224) # <- DO NOT CHANGE\n",
    "\n",
    "# Experiment with batch_size and epochs\n",
    "batch_size=32\n",
    "epochs=15\n",
    "\n",
    "steps_per_epoch = 2000 / batch_size\n",
    "steps_per_epoch_valid = 1000 / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing\n",
    "\n",
    "To help ourselves in loading and processing images, let's use **ImageDataGenerator** provided as a part of the TensorFlow library.\n",
    "\n",
    "To learn more about data generators and how to use them, read this blog:\n",
    "\n",
    "- https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# For this project we will use only scaling as the image preprocessing step (All pixels between 0-1)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Data Generators over the downloaded dataset\n",
    "\n",
    "Using ImageDataGenerators allows us to load images in many ways. In our case, we have all images in the folder called **data**, and each class is its folder *[\"cat,\" \"dog\"]*. This is the perfect setup for the function called **flow_from_directory**! \n",
    "\n",
    "This function takes data from a specified folder and automatically detects the number of images, number of classes and loads them in the memory when the training starts. When defining the generator, you can specify a standardized image size to resize all loaded images to the specified size. \n",
    "\n",
    "Here is the link to learn more about *flow_from_directory*:\n",
    "- https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # This is the source directory for training images\n",
    "        target_size=IMG_SIZE,  # All images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 20 using val_datagen generator\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Train the *model* using all the parameters, **train_generator** and **validation_generator**\n",
    "\n",
    "HINT: Here is the post that explains how to train a model using data generators: https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-b7291d783e6e>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/15\n",
      "63/62 - 16s - loss: 1.2122 - output_loss: 0.7922 - dense_1_loss: 0.7003 - dense_3_loss: 0.6997 - output_acc: 0.5010 - dense_1_acc: 0.5135 - dense_3_acc: 0.5035 - val_loss: 1.1435 - val_output_loss: 0.6988 - val_dense_1_loss: 0.7714 - val_dense_3_loss: 0.7108 - val_output_acc: 0.5000 - val_dense_1_acc: 0.5000 - val_dense_3_acc: 0.5000\n",
      "Epoch 2/15\n",
      "63/62 - 15s - loss: 1.1108 - output_loss: 0.6939 - dense_1_loss: 0.6951 - dense_3_loss: 0.6946 - output_acc: 0.4900 - dense_1_acc: 0.5010 - dense_3_acc: 0.4845 - val_loss: 1.1096 - val_output_loss: 0.6935 - val_dense_1_loss: 0.6938 - val_dense_3_loss: 0.6932 - val_output_acc: 0.5000 - val_dense_1_acc: 0.5000 - val_dense_3_acc: 0.5000\n",
      "Epoch 3/15\n",
      "63/62 - 14s - loss: 1.1104 - output_loss: 0.6938 - dense_1_loss: 0.6942 - dense_3_loss: 0.6944 - output_acc: 0.4860 - dense_1_acc: 0.4940 - dense_3_acc: 0.4940 - val_loss: 1.1091 - val_output_loss: 0.6932 - val_dense_1_loss: 0.6931 - val_dense_3_loss: 0.6933 - val_output_acc: 0.5000 - val_dense_1_acc: 0.5000 - val_dense_3_acc: 0.5000\n",
      "Epoch 4/15\n",
      "63/62 - 13s - loss: 1.1102 - output_loss: 0.6936 - dense_1_loss: 0.6946 - dense_3_loss: 0.6940 - output_acc: 0.4940 - dense_1_acc: 0.5100 - dense_3_acc: 0.5055 - val_loss: 1.1093 - val_output_loss: 0.6931 - val_dense_1_loss: 0.6939 - val_dense_3_loss: 0.6933 - val_output_acc: 0.5000 - val_dense_1_acc: 0.5000 - val_dense_3_acc: 0.5000\n",
      "Epoch 5/15\n",
      "63/62 - 12s - loss: 1.1102 - output_loss: 0.6934 - dense_1_loss: 0.6954 - dense_3_loss: 0.6939 - output_acc: 0.4850 - dense_1_acc: 0.4810 - dense_3_acc: 0.4935 - val_loss: 1.1089 - val_output_loss: 0.6931 - val_dense_1_loss: 0.6928 - val_dense_3_loss: 0.6932 - val_output_acc: 0.5000 - val_dense_1_acc: 0.5000 - val_dense_3_acc: 0.5000\n",
      "Epoch 6/15\n",
      "63/62 - 12s - loss: 1.1111 - output_loss: 0.6938 - dense_1_loss: 0.6966 - dense_3_loss: 0.6941 - output_acc: 0.5230 - dense_1_acc: 0.5150 - dense_3_acc: 0.5120 - val_loss: 1.1097 - val_output_loss: 0.6937 - val_dense_1_loss: 0.6929 - val_dense_3_loss: 0.6938 - val_output_acc: 0.5000 - val_dense_1_acc: 0.5000 - val_dense_3_acc: 0.5000\n",
      "Epoch 7/15\n",
      "63/62 - 12s - loss: 1.1105 - output_loss: 0.6940 - dense_1_loss: 0.6941 - dense_3_loss: 0.6943 - output_acc: 0.4970 - dense_1_acc: 0.5225 - dense_3_acc: 0.5090 - val_loss: 1.1090 - val_output_loss: 0.6933 - val_dense_1_loss: 0.6927 - val_dense_3_loss: 0.6930 - val_output_acc: 0.5000 - val_dense_1_acc: 0.5130 - val_dense_3_acc: 0.5000\n",
      "Epoch 8/15\n",
      "63/62 - 12s - loss: 1.1220 - output_loss: 0.6972 - dense_1_loss: 0.7056 - dense_3_loss: 0.7103 - output_acc: 0.5085 - dense_1_acc: 0.5395 - dense_3_acc: 0.4945 - val_loss: 1.1087 - val_output_loss: 0.6922 - val_dense_1_loss: 0.6961 - val_dense_3_loss: 0.6924 - val_output_acc: 0.5000 - val_dense_1_acc: 0.5000 - val_dense_3_acc: 0.5000\n",
      "Epoch 9/15\n",
      "63/62 - 12s - loss: 1.1266 - output_loss: 0.7021 - dense_1_loss: 0.7067 - dense_3_loss: 0.7084 - output_acc: 0.5255 - dense_1_acc: 0.5370 - dense_3_acc: 0.5260 - val_loss: 1.1054 - val_output_loss: 0.6925 - val_dense_1_loss: 0.6840 - val_dense_3_loss: 0.6925 - val_output_acc: 0.5190 - val_dense_1_acc: 0.5240 - val_dense_3_acc: 0.5050\n",
      "Epoch 10/15\n",
      "63/62 - 12s - loss: 1.1083 - output_loss: 0.6957 - dense_1_loss: 0.6824 - dense_3_loss: 0.6928 - output_acc: 0.5605 - dense_1_acc: 0.5630 - dense_3_acc: 0.5585 - val_loss: 1.0972 - val_output_loss: 0.6879 - val_dense_1_loss: 0.6785 - val_dense_3_loss: 0.6858 - val_output_acc: 0.5650 - val_dense_1_acc: 0.6180 - val_dense_3_acc: 0.5680\n",
      "Epoch 11/15\n",
      "63/62 - 12s - loss: 1.1066 - output_loss: 0.6942 - dense_1_loss: 0.6813 - dense_3_loss: 0.6934 - output_acc: 0.5545 - dense_1_acc: 0.5775 - dense_3_acc: 0.5680 - val_loss: 1.0944 - val_output_loss: 0.6873 - val_dense_1_loss: 0.6739 - val_dense_3_loss: 0.6831 - val_output_acc: 0.5610 - val_dense_1_acc: 0.5950 - val_dense_3_acc: 0.5790\n",
      "Epoch 12/15\n",
      "63/62 - 12s - loss: 1.0933 - output_loss: 0.6865 - dense_1_loss: 0.6715 - dense_3_loss: 0.6843 - output_acc: 0.5665 - dense_1_acc: 0.6025 - dense_3_acc: 0.5765 - val_loss: 1.0880 - val_output_loss: 0.6828 - val_dense_1_loss: 0.6722 - val_dense_3_loss: 0.6785 - val_output_acc: 0.5500 - val_dense_1_acc: 0.6000 - val_dense_3_acc: 0.5500\n",
      "Epoch 13/15\n",
      "63/62 - 12s - loss: 1.0829 - output_loss: 0.6799 - dense_1_loss: 0.6695 - dense_3_loss: 0.6741 - output_acc: 0.5820 - dense_1_acc: 0.6150 - dense_3_acc: 0.5970 - val_loss: 1.0557 - val_output_loss: 0.6654 - val_dense_1_loss: 0.6463 - val_dense_3_loss: 0.6547 - val_output_acc: 0.6270 - val_dense_1_acc: 0.6430 - val_dense_3_acc: 0.6290\n",
      "Epoch 14/15\n",
      "63/62 - 14s - loss: 1.0699 - output_loss: 0.6748 - dense_1_loss: 0.6500 - dense_3_loss: 0.6672 - output_acc: 0.6115 - dense_1_acc: 0.6320 - dense_3_acc: 0.6200 - val_loss: 1.0506 - val_output_loss: 0.6646 - val_dense_1_loss: 0.6367 - val_dense_3_loss: 0.6499 - val_output_acc: 0.5900 - val_dense_1_acc: 0.6540 - val_dense_3_acc: 0.6290\n",
      "Epoch 15/15\n",
      "63/62 - 12s - loss: 1.0692 - output_loss: 0.6728 - dense_1_loss: 0.6567 - dense_3_loss: 0.6645 - output_acc: 0.5875 - dense_1_acc: 0.6425 - dense_3_acc: 0.6130 - val_loss: 1.0304 - val_output_loss: 0.6555 - val_dense_1_loss: 0.6172 - val_dense_3_loss: 0.6324 - val_output_acc: 0.6230 - val_dense_1_acc: 0.6690 - val_dense_3_acc: 0.6460\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch, \n",
    "                              epochs=epochs,\n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=steps_per_epoch_valid,\n",
    "                              verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using trained model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.where(model.predict(validation_generator)[0] < 0.5, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
