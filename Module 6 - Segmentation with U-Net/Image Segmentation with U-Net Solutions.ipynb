{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuclei segmentation using U-Net\n",
    "\n",
    "\n",
    "![Nuclei segmentation](https://pics.spark-in.me/upload/751214b3f2d19ed1cdfcef100d9f8f5a.png)\n",
    "<center>Image taken from <a href=\"https://spark-in.me/post/playing-with-dwt-and-ds-bowl-2018\">here</a></center>\n",
    "<br><br>\n",
    "In this project, we will talk about **object segmentation**, which is a bit different from the task you've done so far. In the object detection task, we have to predict a bounding box that goes around an object at hand, but now we need to make predictions on the pixel level! In other words, object segmentation is a task where the model needs to make predictions for each pixel in an image.\n",
    "\n",
    "One example of object segmentation is pedestrian detection. In self-driving cars, we would like to make predictions of car's surroundings as precise as possible. Having a pixel-level annotation for each object in a frame is a fantastic way to navigate around.\n",
    "\n",
    "![Self driving car segmentation](https://cdn.motor1.com/images/mgl/PKgL4/s6/following-a-fire-engine-with-tesla-autopilot-source-greentheonly.jpg)\n",
    "<center>Image taken from <a href=\"https://insideevs.com/news/396126/tesla-autopilot-neural-network-advancements/\">here</a></center>\n",
    "<br><br>\n",
    "\n",
    "\n",
    "### Steps:\n",
    "1. Import dependencies\n",
    "2. Custom mask generator\n",
    "3. Loading the dataset\n",
    "4. Encoder block\n",
    "5. Decoder block\n",
    "6. U-Net\n",
    "7. Training callbacks\n",
    "8. Model training\n",
    "9. Generating predictions\n",
    "\n",
    "### Topics covered and learning objectives\n",
    "- Object Segmentation\n",
    "- Autoencoders\n",
    "- Generative models\n",
    "- U-Net\n",
    "- Generating images as predictions\n",
    "\n",
    "### Time estimates:\n",
    "- Reading/Watching materials: 3h 20min\n",
    "- Exercises: 1h - 1h 30min\n",
    "<br><br>\n",
    "- **Total**: ~5h\n",
    "\n",
    "\n",
    "To learn more about object segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/XMSjOatyH0k\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x28850b58dc8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/XMSjOatyH0k\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case here is the like for YouTube video from the cell above: https://www.youtube.com/watch?v=XMSjOatyH0k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task introduction\n",
    "\n",
    "![U-Net](https://miro.medium.com/max/700/0*l23lbOWx-_deTkyk.png)\n",
    "\n",
    "Object segmentation is vital for systems that require high precision. Most health-related tasks are in this category. In this example, we will use data from Data Science Bowl 2018 -  Prediction of nuclei. \n",
    "\n",
    "To understand better **why nuclei** here is a paragraph from Kaggle's website:\n",
    "\n",
    "    Identifying the cells' nuclei is the starting point for most analyses because most of the human body's 30 trillion cells contain a nucleus full of DNA. This genetic code programs each cell. Identifying nuclei allows researchers to identify each cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/competitions/dsb-2018/dsb.jpg)\n",
    "<center>Image taken from <a href=\"https://www.kaggle.com/c/data-science-bowl-2018\">here</a></center>\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/Dbiq6l50zO8\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x28854534488>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/Dbiq6l50zO8\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case, here is the like for YouTube video from the cell above: https://www.youtube.com/watch?v=Dbiq6l50zO8\n",
    "\n",
    "\n",
    "### Imports for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PurePath, Path\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import chain\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dropout, Conv2D, Conv2DTranspose, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tests import TEST_UNET_MODEL, TEST_DECODER_BLOCK, TEST_ENCODER_BLOCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "The dataset is an official part of the Kaggle's Data Science Bowl 2018 competition. To download this dataset, you'll need to be logged in using your Kaggle account and accept the competition rules! Link to the competition and data is [here](https://www.kaggle.com/c/data-science-bowl-2018/data)\n",
    "\n",
    "Download it from the *[Data](https://www.kaggle.com/c/data-science-bowl-2018/data)* page by clicking onto **Download all**\n",
    "\n",
    "![](images/download-bowl.png)\n",
    "\n",
    "Once downloaded, your zip file will contain **seven (7)** zip files! We don't need all of them. \n",
    "\n",
    "Extract only these two files:\n",
    "- stage1_train.zip\n",
    "- stage1_test.zip\n",
    "\n",
    "Extract both of them in the working directory or **data/module_6** folder in the root of the repo for the following to work!\n",
    "\n",
    "Once I've done that, I had two sub-folders, which I renamed to **train** and **test**.\n",
    "After extracting everything, this was my folder structure:\n",
    "\n",
    "<pre>\n",
    "<b>module_6</b>\n",
    "|__ <b>train</b>\n",
    "    |______ <b>00ae65c1c6631ae6f2be1a449902976e6eb8483bf6b0740d00530220832c6d3e</b>\n",
    "            |____<b>images</b> 00ae65c1c6631ae6f2be1a449902976e6eb8483bf6b0740d00530220832c6d3e.png\n",
    "            |____<b>masks</b> [mask_img_1.png, mask_img_2.jpg, mask_img_3.png ...]\n",
    "    |______ <b>0a7d30b252359a10fd298b638b90cb9ada3acced4e0c0e5a3692013f432ee4e9</b>\n",
    "            |____<b>images</b>: 0a7d30b252359a10fd298b638b90cb9ada3acced4e0c0e5a3692013f432ee4e9.png\n",
    "            |____<b>masks</b>: [mask_img_1.png, mask_img_2.jpg, mask_img_3.png ...]\n",
    "    |______ <b>...</b>\n",
    "|__ <b>test</b>\n",
    "    |______ <b>0a849e0eb15faa8a6d7329c3dd66aabe9a294cccb52ed30a90c8ca99092ae732</b>\n",
    "            |____<b>images</b> 0a849e0eb15faa8a6d7329c3dd66aabe9a294cccb52ed30a90c8ca99092ae732.png\n",
    "            |____<b>masks</b> [mask_img_1.png, mask_img_2.jpg, mask_img_3.png ...]\n",
    "    |______ <b>0e132f71c8b4875c3c2dd7a22997468a3e842b46aa9bd47cf7b0e8b7d63f0925</b>\n",
    "            |____<b>images</b>: 0e132f71c8b4875c3c2dd7a22997468a3e842b46aa9bd47cf7b0e8b7d63f0925.png\n",
    "            |____<b>masks</b>: [mask_img_1.png, mask_img_2.jpg, mask_img_3.png ...]\n",
    "    |______ <b>...</b>\n",
    "</pre>\n",
    "\n",
    "For each image, there are several mask images (for each cell individually). In the data preprocessing stage, we will need to combine all of these masks to create one single mask per image, acting as the target for our U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (128, 128, 1)\n",
    "\n",
    "# CHANGE THIS IF YOU NAMED YOUR DATA FOLDER DIFFERENTLY\n",
    "REPO_DIR = Path(os.getcwd()).parent\n",
    "\n",
    "# Note: Please put the data into the data folder in the root of the repo for the following to work!\n",
    "TRAIN_PATH = REPO_DIR / \"data/module_6/train\"\n",
    "TEST_PATH = REPO_DIR / \"data/module_6/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Making targets from masks\n",
    "\n",
    "In this Data Science bowl, the authors provided us with a bit of a challenge - to make our targets. For each image, we have between 1 and 300 separate masks! Having tons of images as our targets will make the learning process for U-Net much harder. \n",
    "\n",
    "To overcome this, we will create one single target image, which will be cross-over from all the masks provided for that image.\n",
    "\n",
    "Our original masks look like this:\n",
    "\n",
    "![](images/masks.png)\n",
    "\n",
    "From this, we will end up with this:\n",
    "\n",
    "![](images/final-mask.png)\n",
    "\n",
    "As you can see, the final mask is made from many white spots, where each of these dots was an image for itself.\n",
    "\n",
    "Your task now is to complete the **make_mask_target** function, which does precisely this. \n",
    "\n",
    "Here are the steps to follow:\n",
    "\n",
    "1. Make a matrix of zeros using np.zeros function. This matrix should be the size of IMG_SIZE[0] x IMG_SIZE[1] x 1 \n",
    "    NOTE: dtype is np.bool because we would like to binarize the output for predictions\n",
    "    \n",
    "2. Iterate through all mask images located in the *mask_folder_path* (input of the function)\n",
    "\n",
    "3. Load each mask using OpenCV in the GRAYSCALE reading mode \n",
    "\n",
    "4. Binarize the input using the OTSU threshold technique \n",
    "   - Here is how-to: https://stackoverflow.com/questions/7624765/converting-an-opencv-image-to-black-and-white\n",
    "   \n",
    "\n",
    "5. Resize the image to be (IMG_SIZE[0], IMG_SIZE[1])\n",
    "\n",
    "6. Expend dimensions on using np.expand_dims(matrix, axis=-1)\n",
    "\n",
    "7. Merge the matrix created in step 1 with the matrix from step 6 using np.maximum\n",
    "    - Using this method will take maximum values for all places in the matrix, preserving all masks already merged in the process\n",
    "    \n",
    "\n",
    "8. Return matrix from step 1 with all masks merged into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask_target(mask_folder_path):\n",
    "    \"\"\"\n",
    "    Creates target images from many nucleai masks.\n",
    "    \n",
    "    Args:\n",
    "        mask_folder_path :string: - path to the folder that contains all the mask for an input image\n",
    "    \"\"\"\n",
    "    target = np.zeros((IMG_SIZE[0], IMG_SIZE[1], 1), dtype=np.bool)\n",
    "    \n",
    "    for mask_candidate in os.listdir(masks_folder):\n",
    "        mask_candidate = cv2.imread(os.path.join(masks_folder, mask_candidate), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # EXERCISE \n",
    "        (thresh, im_bw) = cv2.threshold(mask_candidate, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "        mask_candidate = cv2.resize(im_bw, (IMG_SIZE[0], IMG_SIZE[1]), cv2.INTER_LINEAR)\n",
    "        \n",
    "        mask_candidate = np.expand_dims(mask_candidate, axis=-1)\n",
    "        target = np.maximum(target, mask_candidate)\n",
    "        \n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce78a4cdb03404aa85ef513d8cabf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=670.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i, folder in tqdm(enumerate(os.listdir(TRAIN_PATH.__str__())), total=len(os.listdir(TRAIN_PATH.__str__()))):\n",
    "    path = TRAIN_PATH / folder\n",
    "    img_folder =  path / \"images\"\n",
    "    masks_folder = path / \"masks\"\n",
    "    \n",
    "    # Load image in the GrayScale format\n",
    "    img = cv2.imread((img_folder / (path.parts[-1] + \".png\")).__str__(), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Resize image to the IMG_SIZE \n",
    "    img = cv2.resize(img, (IMG_SIZE[0], IMG_SIZE[1]), cv2.INTER_LINEAR)\n",
    "    X_train.append(img)\n",
    "    \n",
    "    # Creating the target mask\n",
    "    target = make_mask_target(masks_folder)\n",
    "    \n",
    "    y_train.append(target / 255.0)\n",
    "    \n",
    "X_train = np.expand_dims(np.array(X_train), -1)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test images to check model preformance at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22a8e82891c49808127316fd127ee73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=65.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get and resize test images\n",
    "X_test = []\n",
    "\n",
    "for i, folder in tqdm(enumerate(os.listdir(TEST_PATH.__str__())), total=len(os.listdir(TEST_PATH.__str__()))):\n",
    "    path = TEST_PATH / folder\n",
    "    img_folder =  path / \"images\"\n",
    "    \n",
    "    # Load image in he RGB format\n",
    "    img = cv2.imread((img_folder / (path.parts[-1] + \".png\")).__str__(), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img = cv2.resize(img, (IMG_SIZE[0], IMG_SIZE[1]), cv2.INTER_LINEAR)\n",
    "    X_test.append(img)\n",
    "    \n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 128, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Encoder block\n",
    "\n",
    "Before we start building the architecture, it is important to learn the underlying concet of U-Net - Autoencoders! \n",
    "\n",
    "Intro to autoencoders: https://www.jeremyjordan.me/autoencoders/\n",
    "Examples of AE applications: https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726\n",
    "\n",
    "\n",
    "In this exercise, you'll complete the **encoder_block** function. This function will be used to create a single block in the first part of the network. \n",
    "\n",
    "![](https://miro.medium.com/max/384/1*yGu1oXPeqEvbKRuWL0z3Dw.png)\n",
    "\n",
    "It accepts two arguments *inputs* from a previous layer in the network, and the second one is *features* used for Conv2D layers.\n",
    "\n",
    "Each encoder block has **four (4)** parts.\n",
    "1. Conv2D layer, which gets the number of features from the argument of the function, 3x3 kernel, and activation is ReLu. Padding is \"same\"\n",
    "2. Dropout layer with 0.2 drop rate\n",
    "3. Conv2d layer with the same set of arguments as the first convolutional layer\n",
    "4. Finish the encoder block with MaxPooling2d layer with a pool size of 2x2\n",
    "\n",
    "NOTE: This function should return **TWO** things\n",
    "    - Output from the 2nd Conv2d layer (output from this layer will be used to concatenate with decoder layers)\n",
    "    - Output from the MaxPooling layer (this is the regular output used as a previous layer in the network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(X, features):\n",
    "    \"\"\"\n",
    "    Create an encoder_block (layer) for the U-Net\n",
    "    \n",
    "    Args:\n",
    "        X - input from the previous layer\n",
    "        features :int: - number of features used in Conv2D layers\n",
    "    \"\"\"\n",
    "    X = Conv2D(features, (3, 3), activation='relu', padding='same') (X)\n",
    "    X = Dropout(0.2) (X)\n",
    "    enX = Conv2D(features, (3, 3), activation='relu', padding='same') (X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2)) (X)\n",
    "    return X, enX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:green>IMPLEMENTATION IS CORRECT! GOOD JOB!</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE\n",
    "TEST_ENCODER_BLOCK(encoder_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Decoder block\n",
    "\n",
    "In this exercise, you'll complete the **decoder_block** function. This function will be used to create a single block in the second part of the network. \n",
    "\n",
    "![](images/decoder-illustration.png)\n",
    "\n",
    "The decoder block accepts 3 arguments *inputs* from a previous layer in the network, output from a corresponding encoder layer, and *features* used for Conv2D layers.\n",
    "\n",
    "Each Decoder block has **five (4)** parts/steps\n",
    "\n",
    "1. Start with Conv2DTranspose with the number of features equal to the argument from the function, 2x2 kernel, 2x2 strides, and padding is equal to \"same\"\n",
    "    - This layer is used to increase the size of the input by two (e.g., if the input is 16x16, the output will be 32x32)\n",
    "2. The second step is the most important step for the U-Net architecture - Concatenating corresponding encoder output with step 1. HINT: use tf.concat([], axis=3)\n",
    "3. Conv2D layer, which gets the number of features from the argument of the function, 3x3 kernel, and activation is ReLu. Padding is \"same\"\n",
    "4. Dropout layer with 0.2 drop rate\n",
    "5. Conv2d layer with the same set of arguments as the Conv2D from step 3\n",
    "\n",
    "Learning resources:\n",
    "- Transposed convolutions https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(X, encoder_pair, features):\n",
    "    \"\"\"\n",
    "    Create an decoder_block (layer) for the U-Net\n",
    "    \n",
    "    Args:\n",
    "        X - input from the previous layer\n",
    "        encoder_paier - Output from a corresponding encoder layer\n",
    "        features :int: - number of features used in Conv2D and Conv2DTranspose layers\n",
    "    \"\"\"\n",
    "    X = Conv2DTranspose(features, (2, 2), strides=(2, 2), padding='same') (X)\n",
    "\n",
    "    X = tf.concat([X, encoder_pair], axis=3)\n",
    "    X = Conv2D(features, (3, 3), activation='relu', padding='same') (X)\n",
    "    X = Dropout(0.2) (X)\n",
    "    X = Conv2D(features, (3, 3), activation='relu', padding='same') (X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:green>IMPLEMENTATION IS CORRECT! GOOD JOB!</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE\n",
    "TEST_DECODER_BLOCK(decoder_block, encoder_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 Complete the U-Net architecture\n",
    "\n",
    "On the image here, you can see the architecture we are building here.\n",
    "\n",
    "![](images/u-net.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/azM57JuQpQI\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2887dffb208>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/azM57JuQpQI\", 1000, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case, here is the like for YouTube video from the cell above: https://www.youtube.com/watch?v=azM57JuQpQI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/oLvmLJkmXuc\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2887e009508>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/oLvmLJkmXuc\", 1000,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In some cases, IPython widgets do not work!**\n",
    "\n",
    "If this is the case, here is the like for YouTube video from the cell above: https://www.youtube.com/watch?v=oLvmLJkmXuc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understanding Semantic Segmentation with UNET https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47\n",
    "\n",
    "In the cell below, fill in the missing parts to get the same architecture as represented in the picture. Inputs, the middle part, and the output layer are already given to you.\n",
    "\n",
    "Encoder hints:\n",
    "1. Encoder is made out of 4 **encoder_blocks** \n",
    "    - First one has 16 features\n",
    "    - Second one, 32, features\n",
    "    - Third one, 64 \n",
    "    - Fourth block has 128\n",
    "\n",
    "2. Each encoder block returns two values, real outputs and outputs used to concatenate with corresponding decoder block\n",
    "3. Decoder block has  4 **decoder_blocks**\n",
    "    - First has 128, and this block is paired with encoder block 4\n",
    "    - Second block has 64, and it's paired with 3rd encoder block\n",
    "    - Third block has 32, and corresponding encoder block is the 2nd one\n",
    "    - Fourth one has 16 and pairs it with the first encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((128, 128,1))\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "X, enc1 = encoder_block(inputs, 16)\n",
    "X, enc2 = encoder_block(X, 32)\n",
    "X, enc3 = encoder_block(X, 64)\n",
    "X, enc4 = encoder_block(X, 128)\n",
    "\n",
    "mid = Conv2D(256, (3, 3), activation='relu', padding='same') (X)\n",
    "mid = Dropout(0.3) (mid)\n",
    "mid = Conv2D(256, (3, 3), activation='relu', padding='same') (mid)\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "dec1 = decoder_block(mid, enc4, 128)\n",
    "dec2 = decoder_block(dec1, enc3, 64)\n",
    "dec3 = decoder_block(dec2, enc2, 32)\n",
    "dec4 = decoder_block(dec3, enc1, 16)\n",
    "\n",
    "# don't forget to add output from the last decoder alyer to the output layer\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (dec4)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:green>IMPLEMENTATION IS CORRECT! GOOD JOB!</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL TO TEST YOUR CODE\n",
    "TEST_UNET_MODEL(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (Optional) Model callbacks\n",
    "\n",
    "Define **EarlyStopping** callback. You can use arguments of choice here\n",
    "Define **ModelCheckpoint** callback, use save_best_only=True, choose whatever name you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping_callback = EarlyStopping(patience=5, verbose=1)\n",
    "checkpoint_callback = ModelCheckpoint('model-dsbowl2018-1.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 Train the model\n",
    "\n",
    "Call the **fit** function on the model and provide all the necessary arguments. In the cell below, you'll find already pre-defined hyperparameters for training, but try to experiment with them.\n",
    "\n",
    "NOTE: If you choose to solve **Exercise 4** don't forget to put them in the fit function \n",
    "- https://keras.io/api/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16 \n",
    "EPOCHS=20\n",
    "VALIDATION_SPLIT=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "                    callbacks=[earlystopper, checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting using the trained model\n",
    "\n",
    "We call **model.predict** and provide the **X_test** set to it to make a prediction. Our model will make predictions going from 0 to 1, but we need to binarize the output. To do that, we can simply add **> 0.5** after that, which will make the prediction 1 (True) or 0 (False) for this comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, verbose=1) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = random.randint(0, len(preds))\n",
    "plt.imshow(X_test[ix], cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(np.squeeze(preds[ix]), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading homework - Mask R-CNN\n",
    "\n",
    "\n",
    "Now that you know how U-Net and R-CNN work, it's time to read about an R-CNN version used for semantic segmentation called Masked R-CNN. We won't be implementing it here, but you'll have a lot of resources that go through code parts.\n",
    "\n",
    "\n",
    "### Topics covered and learning objectives\n",
    "- Mask R-CNN\n",
    "\n",
    "### Time estimates:\n",
    "- Reading/Watching materials: 1h\n",
    "<br><br>\n",
    "- **Total**: ~1h\n",
    "\n",
    "\n",
    "\n",
    "- Simple introduction to Mask R-CNN https://alittlepain833.medium.com/simple-understanding-of-mask-rcnn-134b5b330e95 \n",
    "- Video introduction to semantic segmentation https://www.youtube.com/watch?v=4tkgOzQ9yyo\n",
    "- Get deeper into Mask R-CNN with pieces of code https://towardsdatascience.com/computer-vision-instance-segmentation-with-mask-r-cnn-7983502fcad1\n",
    "- Keras and Mask R-CNN https://machinelearningmastery.com/how-to-perform-object-detection-in-photographs-with-mask-r-cnn-in-keras/\n",
    "- Implementation and project with Mask R-CNN https://towardsdatascience.com/webcam-object-detection-with-mask-r-cnn-on-google-colab-b3b012053ed1\n",
    "\n",
    "Try it out in the cloud:\n",
    "https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
